python -m torch.distributed.run --nnodes=1 --nproc_per_node=1 --module se3_transformer.run_mae.training --amp true  --batch_size 240 --epochs 100 --lr 0.002 --weight_decay 0.1 --use_layer_norm --norm --save_ckpt_path trained_models/ptm/mae_pos.pth --precompute_bases --seed 42 --task homo --wandb y --ckpt_interval 50 --ep_name mae_pos_803 --loss_type mse 
rlaunch --cpu=20 --gpu=1 --memory=80000 --private-machine=project --enable-sshd=true -- python /home/sunyuancheng/molmae/se3_transformer/run_mae/finetune.py --amp true --batch_size 128 --epochs 100 --lr 0.002 --weight_decay 0.1 --use_layer_norm --norm --save_ckpt_path ~/molmae/trained_models/alpha/pos_lr000002_wd0_mr03.pth --precompute_bases --seed 42 --task alpha --wandb y --ckpt_interval 50 --eval_interval 1 --loss_type mae --pooling avg --exp_name alpha_pos_lr000002_wd0_mr03 --split_size_type finetune --load_ptm_path  ~/molmae/trained_models/ptm/pos_lr000002_wd0_mr03.pth

