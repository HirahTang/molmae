python -m torch.distributed.run --nnodes=1 --nproc_per_node=gpu /home/sunyuancheng/molmae/se3_transformer/run_mae/training.py --amp true --batch_size 240 --epochs 100 --lr 0.0002 --weight_decay 0.1 --use_layer_norm --norm --save_ckpt_path trained_models/ptm/pos_lr00002.pth --precompute_bases --seed 42 --task homo --wandb y --ckpt_interval 50 --exp_name pos_lr00002 --loss_type mse --mask_ratio 0.1